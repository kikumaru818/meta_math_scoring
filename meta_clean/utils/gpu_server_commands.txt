srun --pty -p rtx8000-short --mem=32000 --ntasks-per-node=8 --gres=gpu:1 bash
srun --pty -p 2080ti-long --mem=32000 --ntasks-per-node=4 --gres=gpu:4  bash
srun --pty -p 1080ti-long --mem=32000 --ntasks-per-node=4 --gres=gpu:4 --exclude=node108 bash
srun --pty -p rtx8000-long --mem=32000 --ntasks-per-node=8 --gres=gpu:1 bash
srun --pty -p 2080ti-short --mem=32000 --ntasks-per-node=4 --gres=gpu:4 bash


Number of GPUs available per user:
sacctmgr show assoc where user=nigel format="partition%-15,grptres"

GPUs currently used:
squeue -u nigel

Unity partition info:
sinfo -o"%P %.5a %.10l %.6D %.6t %N %f"


macbook:

"in_context_tuning"
python train.py\
    --name "test_lm_in_context_answer_only"\
    --task "Grade 4/2017_DBA_DR04_1715RE4T05G04_06"\
    --lm "bert-base-uncased"\
    --batch_size 16\
    --eval_freq 1\
    --neptune_project "ni9elf/naep"\
    --workers 0\
    --debug --iters 1\
    --in_context_tuning\
    --num_test_avg 2\
    --neptune

"meta"
python train.py\
    --name "test_lm_meta_answer_only"\
    --task "meta"\
    --lm "bert-base-uncased"\
    --batch_size 16\
    --eval_freq 1\
    --neptune_project "ni9elf/naep"\
    --workers 0\
    --debug --iters 2\
    --meta_learning\
    --num_test_avg 2\
    --neptune\
    --generative_model
    

gypsum:

python train.py\
    --name "answer_only"\
    --task "Grade 4/2017_DBA_DR04_1715RE1T10_05"\
    --lm "microsoft/deberta-v2-xlarge"\
    --batch_size 24\
    --neptune_project "ni9elf/naep"\
    --eval_freq 1\
    --cuda --neptune

python train.py\
    --name "with_question_and_sep"\
    --task "Grade 4/2017_DBA_DR04_1715RE1T10_05"\
    --lm "allenai/longformer-base-4096"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --eval_freq 1\
    --cuda --neptune


python train.py\
    --name "spell_check_answer_only"\
    --task "Grade 4/2017_DBA_DR04_1715RE4T05G04_06"\
    --lm "bert-base-uncased"\
    --batch_size 8\
    --eval_freq 1\
    --cuda\
    --neptune_project "ni9elf/naep"\
    --neptune --spell_check


python train.py\
    --name "in_context_tuning_answer_only"\
    --task "Grade 4/2017_DBA_DR04_1715RE4T05G04_06"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --cuda --neptune \
    --in_context_tuning\
    --num_test_avg 32


"test_in_context_tuning"
python train.py\
    --name "test_in_context_tuning_answer_only"\
    --task "Grade 4/2017_DBA_DR04_1715RE4T05G04_06"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --cuda --neptune \
    --in_context_tuning\
    --num_test_avg 32\
    --debug

"test_lm_base"
python train.py\
    --name "test_lm_base_answer_only"\
    --task "Grade 4/2017_DBA_DR04_1715RE4T05G04_06"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --eval_freq 1\
    --neptune_project "ni9elf/naep"\
    --cuda --neptune\
    --debug 

"test_meta"
python train.py\
    --name "test_lm_meta_answer_question"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --cuda --neptune\
    --debug


python train.py\
    --name "test_lm_base_amp"\
    --task "Grade 8/2017_DBA_DR08_1715RE4T08G08_06"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --cuda --neptune


python train.py\
    --name "lm_meta_answer_question"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --cuda --neptune

python train.py\
    --name "lm_meta_answer_question"\
    --task "meta_learning_via_in_context_tuning"\
    --batch_size 16\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --cuda --neptune\
    --generative_model --amp


->RAM per GPU:
rtx8000 = 48GB
m40 = 24GB
titanx = 12GB
1080ti = 12GB
2080ti = 12GB


->max GPUs allowed:
rtx8000-long       gres/gpu=2
rtx8000-short      gres/gpu=4
2080ti-long       gres/gpu=20
2080ti-short      gres/gpu=40
1080ti-long       gres/gpu=40
1080ti-short      gres/gpu=88
m40-long          gres/gpu=12
m40-short         gres/gpu=20
titanx-long       gres/gpu=40
titanx-short      gres/gpu=80


->submission train commands on local macbook debugging:

use data splits train-val:
python train.py\
    --name "lm_meta_answer_question_submission"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --data_folder data_split_answer_spell_checked_submission\
    --neptune\
    --workers 0\
    --debug --iters 2

use all data in train:
python train.py\
    --name "lm_meta_answer_question_use_all_data_submission"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --data_folder data_split_answer_spell_checked_use_all_data_submission\
    --neptune\
    --workers 0\
    --debug --iters 2



->submission train commands on gypsum server:

srun --pty -p rtx8000-long --mem=32000 --ntasks-per-node=8 --gres=gpu:1 bash
use data splits train-val:
python train.py\
    --name "lm_meta_answer_question_submission"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --data_folder data_split_answer_spell_checked_submission\
    --cuda --neptune


srun --pty -p rtx8000-long --mem=32000 --ntasks-per-node=8 --gres=gpu:1 bash
use all data in train:
python train.py\
    --name "lm_meta_answer_question_use_all_data_submission"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --data_folder data_split_answer_spell_checked_use_all_data_submission\
    --cuda --neptune



4 separate models on 4 unique tasks
    "Grade 4/2017_DBA_DR04_1715RE1T10_05",
    "Grade 8/2017_DBA_DR08_1715RE2T13_05",
    "Grade 8/2017_DBA_DR08_1715RE2T13_07",
    "Grade 8/2017_DBA_DR08_1715RE2T13_08"

python train.py\
    --name "lm_meta_answer_question_submission"\
    --task "Grade 4/2017_DBA_DR04_1715RE1T10_05"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --data_folder data_split_answer_spell_checked_submission\
    --cuda --neptune\
    --meta_learning_single

python train.py\
    --name "lm_meta_answer_question_submission"\
    --task "Grade 8/2017_DBA_DR08_1715RE2T13_05"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --data_folder data_split_answer_spell_checked_submission\
    --cuda --neptune\
    --meta_learning_single

python train.py\
    --name "lm_meta_answer_question_submission"\
    --task "Grade 8/2017_DBA_DR08_1715RE2T13_07"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --data_folder data_split_answer_spell_checked_submission\
    --cuda --neptune\
    --meta_learning_single

python train.py\
    --name "lm_meta_answer_question_submission"\
    --task "Grade 8/2017_DBA_DR08_1715RE2T13_08"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --data_folder data_split_answer_spell_checked_submission\
    --cuda --neptune\
    --meta_learning_single




Feb 1, 2022

-> AIED Paper Results:

Spell checking commands:

srun --pty -p rtx8000-short --mem=16000 --ntasks-per-node=4 --gres=gpu:1  --exclude=node174,node150,node157,node190 bash
srun --pty -p rtx8000-long --mem=16000 --ntasks-per-node=4 --gres=gpu:1  --exclude=node174,node150,node157,node190 bash
srun --pty -p 2080ti-long --mem=16000 --ntasks-per-node=4 --gres=gpu:1  --exclude=node174,node150,node157,node190 bash
srun --pty -p 1080ti-long --mem=16000 --ntasks-per-node=4 --gres=gpu:1  --exclude=node174,node150,node157,node190 bash
srun --pty -p titanx-long --mem=16000 --ntasks-per-node=4 --gres=gpu:1  --exclude=node174,node150,node157,node190 bash
srun --pty -p m40-long --mem=16000 --ntasks-per-node=4 --gres=gpu:1  --exclude=node174,node150,node157,node190 bash


module load python3/current && cd /home/nigel/naep/code/repo_nigel && source ../../venv/bin/activate

python -m utils.spell_check_dataset --task "Grade 4/2017_DBA_DR04_1715RE1T10_05" d c
python -m utils.spell_check_dataset --task "Grade 4/2017_DBA_DR04_1715RE4T05G04_03" d c
python -m utils.spell_check_dataset --task "Grade 4/2017_DBA_DR04_1715RE4T05G04_06" d c
python -m utils.spell_check_dataset --task "Grade 4/2017_DBA_DR04_1715RE4T05G04_07" d c
python -m utils.spell_check_dataset --task "Grade 4/2017_DBA_DR04_1715RE4T05G04_09" d c
python -m utils.spell_check_dataset --task "Grade 4/2017_DBA_DR04_1715RE4T08G04_03" d c
python -m utils.spell_check_dataset --task "Grade 4/2017_DBA_DR04_1715RE4T08G04_06" d c
python -m utils.spell_check_dataset --task "Grade 4/2017_DBA_DR04_1715RE4T08G04_07" d c
python -m utils.spell_check_dataset --task "Grade 4/2017_DBA_DR04_1715RE4T08G04_09" d c
python -m utils.spell_check_dataset --task "Grade 8/2017_DBA_DR08_1715RE2T13_05" d c
python -m utils.spell_check_dataset --task "Grade 8/2017_DBA_DR08_1715RE2T13_07" d c
python -m utils.spell_check_dataset --task "Grade 8/2017_DBA_DR08_1715RE2T13_08" d c
python -m utils.spell_check_dataset --task "Grade 8/2017_DBA_DR08_1715RE4T05G08_03" d c
python -m utils.spell_check_dataset --task "Grade 8/2017_DBA_DR08_1715RE4T05G08_06" d c 
python -m utils.spell_check_dataset --task "Grade 8/2017_DBA_DR08_1715RE4T05G08_07" d c
python -m utils.spell_check_dataset --task "Grade 8/2017_DBA_DR08_1715RE4T05G08_09" d c
python -m utils.spell_check_dataset --task "Grade 8/2017_DBA_DR08_1715RE4T08G08_03" d c
python -m utils.spell_check_dataset --task "Grade 8/2017_DBA_DR08_1715RE4T08G08_06" d c
python -m utils.spell_check_dataset --task "Grade 8/2017_DBA_DR08_1715RE4T08G08_07" d c
python -m utils.spell_check_dataset --task "Grade 8/2017_DBA_DR08_1715RE4T08G08_09" d c





-> Final Cross validation results - AIED 2022:


# Meta BERT with In-context Tuning:

->cross vall train commands on local macbook debugging:

python train.py\
    --name "lm_meta_answer_question_cross_val_2"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --num_val_avg 8\
    --data_folder data_split_answer_spell_checked\
    --neptune\
    --workers 0\
    --cross_val_fold 2\
    --debug --iters 2





-> cross vall train commands on gypsum server 

srun --pty -p rtx8000-long --mem=32000 --ntasks-per-node=8 --gres=gpu:1 bash
module load python3/current && cd /home/nigel/naep/code/repo_nigel && source ../../venv/bin/activate
(tmux session = 2)
python train.py\
    --name "lm_meta_answer_question_cross_val_1"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --num_val_avg 8\
    --data_folder data_split_answer_spell_checked\
    --cross_val_fold 1\
    --cuda --neptune


srun --pty -p rtx8000-long --mem=32000 --ntasks-per-node=8 --gres=gpu:1 bash
module load python3/current && cd /home/nigel/naep/code/repo_nigel && source ../../venv/bin/activate
(tmux session = 4)
python train.py\
    --name "lm_meta_answer_question_cross_val_2"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --num_val_avg 8\
    --data_folder data_split_answer_spell_checked\
    --cross_val_fold 2\
    --cuda --neptune


srun --pty -p rtx8000-long --mem=32000 --ntasks-per-node=8 --gres=gpu:1 bash
module load python3/current && cd /mnt/nfs/work1/andrewlan/nigel/naep-as-challenge/code/repo_nigel && source ../../venv/bin/activate
python train.py\
    --name "lm_meta_answer_question_cross_val_3"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --num_val_avg 8\
    --data_folder data_split_answer_spell_checked\
    --cross_val_fold 3\
    --cuda --neptune


srun --pty -p rtx8000-long --mem=32000 --ntasks-per-node=8 --gres=gpu:1 bash
module load python3/current && cd /mnt/nfs/work1/andrewlan/nigel/naep-as-challenge/code/repo_nigel && source ../../venv/bin/activate
python train.py\
    --name "lm_meta_answer_question_cross_val_4"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --num_val_avg 8\
    --data_folder data_split_answer_spell_checked\
    --cross_val_fold 4\
    --cuda --neptune


srun --pty -p rtx8000-long --mem=32000 --ntasks-per-node=8 --gres=gpu:1 bash
module load python3/current && cd /mnt/nfs/work1/andrewlan/nigel/naep-as-challenge/code/repo_nigel && source ../../venv/bin/activate
python train.py\
    --name "lm_meta_answer_question_cross_val_5"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --num_val_avg 8\
    --data_folder data_split_answer_spell_checked\
    --cross_val_fold 5\
    --cuda --neptune






-> qualitative error analysis command:

python -m utils.predict_qualitative_error\
    --num_test_avg 8\
    --data_folder data_split_answer_spell_checked_submission\
    --cuda




-> train meta model with demographic information on cross val fold = 1
    
local macbook:

python train.py\
    --name "lm_meta_answer_question_cross_val_1_with_demographic"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --num_val_avg 8\
    --data_folder data_split_answer_spell_checked\
    --neptune\
    --workers 0\
    --cross_val_fold 1\
    --debug --iters 2\
    --demographic


server: 

srun --pty -p rtx8000-long --mem=32000 --ntasks-per-node=8 --gres=gpu:1 bash
module load python3/current && cd /mnt/nfs/work1/andrewlan/nigel/naep-as-challenge/code/repo_nigel && source ../../venv/bin/activate
(tmux session = 2)
python train.py\
    --name "lm_meta_answer_question_cross_val_1_with_demographic"\
    --task "meta_learning_via_in_context_tuning"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning\
    --num_test_avg 8\
    --num_val_avg 8\
    --data_folder data_split_answer_spell_checked\
    --cross_val_fold 1\
    --demographic\
    --cuda --neptune






-> BERT with In-context Tuning (through meta single command): 


local macbook:

python train.py\
    --name "lm_meta_single_answer_question_cross_val"\
    --task "Grade 4/2017_DBA_DR04_1715RE1T10_05"\
    --lm "bert-base-uncased"\
    --batch_size 32\
    --neptune_project "ni9elf/naep"\
    --meta_learning_single\
    --num_test_avg 8\
    --num_val_avg 8\
    --data_folder data_split_answer_spell_checked\
    --cross_val_fold 1\
    --debug --iters 2\
    --workers 0\
    --neptune



server template: 

srun --pty -p 2080ti-short --mem=32000 --ntasks-per-node=4 --gres=gpu:1 --exclude=node094,node097 bash
module load python3/current && cd /home/nigel/naep/code/repo_nigel && source ../../venv/bin/activate
python3 train.py\
    --name "lm_meta_single_answer_question_cross_val"\
    --task "Grade 4/2017_DBA_DR04_1715RE1T10_05"\
    --lm "bert-base-uncased"\
    --batch_size 8\
    --neptune_project "ni9elf/naep"\
    --meta_learning_single\
    --num_test_avg 8\
    --num_val_avg 8\
    --data_folder data_split_answer_spell_checked\
    --cross_val_fold 1\
    --cuda --neptune

ran as SBATCH template for each of the 20 tasks for each fold = 20*5 = 100 runs